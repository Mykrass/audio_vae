{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Recognizing with VAE features!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Importing the libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import Conv2D, Flatten, Lambda\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.models import Model, Sequential\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Loading data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ds/DataScience/Datasets/GoogleSpeechCommands/_commands_'\n",
    "percent_test = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_dict = {}\n",
    "for root, directories, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        label = os.path.split(root)[1]\n",
    "        if label in filenames_dict:\n",
    "            filenames_dict[label] += [os.path.join(root,filename)]\n",
    "        else:\n",
    "            filenames_dict[label] = [os.path.join(root,filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = {label: len(filenames_dict[label]) for label in filenames_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = []\n",
    "test_filenames = []\n",
    "for label in label_counts:\n",
    "    n_test = int(label_counts[label] * 0.2)\n",
    "    filenames = list(zip(filenames_dict[label], [label]*label_counts[label]))\n",
    "    random.shuffle(filenames)\n",
    "    train_filenames += filenames[:-n_test]\n",
    "    test_filenames += filenames[-n_test:]\n",
    "random.shuffle(train_filenames)\n",
    "random.shuffle(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2labels = {label: i for i, label in enumerate(label_counts.keys())}\n",
    "labels2texts = {i: label for i, label in enumerate(label_counts.keys())}\n",
    "num_classes = len(label_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/ds/DataScience/Datasets/GoogleSpeechCommands/_commands_/wow/b6091c84_nohash_0.wav',\n",
       "  'wow'),\n",
       " ('/home/ds/DataScience/Datasets/GoogleSpeechCommands/_commands_/nine/7f74626f_nohash_0.wav',\n",
       "  'nine'),\n",
       " ('/home/ds/DataScience/Datasets/GoogleSpeechCommands/_commands_/nine/c120e80e_nohash_5.wav',\n",
       "  'nine')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filenames[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Defining the architecture for the VAE_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (129, 48, 1)\n",
    "intermediate_dim = 512\n",
    "latent_dim = 40\n",
    "batch_size = 16\n",
    "kernel_size = 6\n",
    "stride_size = 3\n",
    "filters = 16\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "for i in range(2):\n",
    "    filters *= 2\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               activation='tanh',\n",
    "               strides=3,\n",
    "               padding='valid')(x)\n",
    "\n",
    "# shape info needed to build decoder model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# generate latent vector Q(z|X)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate_dim, activation='tanh')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation='tanh')(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "for i in range(2):\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        activation='tanh',\n",
    "                        strides=3,\n",
    "                        padding='valid')(x)\n",
    "    filters //= 2\n",
    "\n",
    "outputs = Conv2DTranspose(filters=1,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='sigmoid',\n",
    "                          padding='same',\n",
    "                          name='decoder_output')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "\n",
    "reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -5e-4\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights('/home/ds/DataScience/Models/audio_vae/40_6_3/4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = pickle.load(open('../data/x_mean.pkl', 'rb'))\n",
    "x_std = pickle.load(open('../data/x_std.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(filename, frame_duration=0.01, stride=0.1, window_size=0.3):\n",
    "    sample_rate, samples = wavfile.read(filename)\n",
    "    frame_size = int(round(frame_duration * float(sample_rate)))\n",
    "    overlap_size = frame_size - int(round((frame_duration - 0.005) * float(sample_rate)))\n",
    "    n_fft_points = 2\n",
    "    while n_fft_points < frame_size:\n",
    "        n_fft_points *= 2\n",
    "    frequencies, times, spectrogram = signal.spectrogram(\n",
    "        samples, fs=sample_rate, window='hamming', nperseg=frame_size, noverlap=overlap_size, nfft=n_fft_points,\n",
    "        scaling='spectrum', mode='psd'\n",
    "    )\n",
    "    n_frames_window = int(frame_size * window_size)\n",
    "    new_features = []\n",
    "    padded_spectrogram = np.hstack((np.zeros((frequencies.shape[0], int(frame_size*stride))),\n",
    "                                    spectrogram,\n",
    "                                    np.zeros((frequencies.shape[0], int(frame_size*stride)))))\n",
    "    for suck_i, time in enumerate(times*stride):\n",
    "        i = int(suck_i/stride)\n",
    "        chunk = padded_spectrogram[:, i:int(frame_size * window_size) + i]\n",
    "        if chunk.shape == (frequencies.shape[0], int(frame_size*window_size)):\n",
    "            new_features += [chunk]\n",
    "    x_file = (np.stack(new_features) - x_mean) / x_std\n",
    "    return encoder.predict(x_file.reshape(x_file.shape + (1,)),\n",
    "                           batch_size=batch_size)[2].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = '/home/ds/DataScience/Datasets/GoogleSpeechCommands/commands_vae_features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 13min 17s, sys: 20.1 s, total: 3h 13min 37s\n",
      "Wall time: 38min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = []\n",
    "y_train = []\n",
    "for filename, label in train_filenames:\n",
    "    x_file = compute_features(filename)\n",
    "    if x_file.shape == (760,):\n",
    "        x_train += [x_file]\n",
    "        y_train += [to_categorical([texts2labels[label]], num_classes=num_classes)[0]]\n",
    "np.stack(x_train).dump(os.path.join(features_path, 'x_train'))\n",
    "np.stack(y_train).dump(os.path.join(features_path, 'y_train'))\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for filename, label in test_filenames:\n",
    "    x_file = compute_features(filename)\n",
    "    if x_file.shape == (760,):\n",
    "        x_test += [x_file]\n",
    "        y_test += [to_categorical([texts2labels[label]], num_classes=num_classes)[0]]\n",
    "np.stack(x_test).dump(os.path.join(features_path, 'x_test'))\n",
    "np.stack(y_test).dump(os.path.join(features_path, 'y_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = np.stack(x_train).shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Defining architecture for MLP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(os.path.join(features_path, 'x_train'))\n",
    "y_train = np.load(os.path.join(features_path, 'y_train'))\n",
    "\n",
    "x_test = np.load(os.path.join(features_path, 'x_test'))\n",
    "y_test = np.load(os.path.join(features_path, 'y_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46630, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = Sequential()\n",
    "asr.add(Dense(100, activation='relu', input_shape=(input_shape,)))\n",
    "asr.add(Dropout(0.2))\n",
    "asr.add(Dense(100, activation='relu'))\n",
    "asr.add(Dropout(0.2))\n",
    "asr.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "asr.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46630 samples, validate on 11630 samples\n",
      "Epoch 1/30\n",
      "46630/46630 [==============================] - 2s 38us/step - loss: 3.1242 - acc: 0.1456 - val_loss: 2.4265 - val_acc: 0.2863\n",
      "Epoch 2/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 2.4591 - acc: 0.2748 - val_loss: 2.1102 - val_acc: 0.3731\n",
      "Epoch 3/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 2.2308 - acc: 0.3384 - val_loss: 1.9457 - val_acc: 0.4182\n",
      "Epoch 4/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 2.1174 - acc: 0.3694 - val_loss: 1.8678 - val_acc: 0.4354\n",
      "Epoch 5/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 2.0456 - acc: 0.3922 - val_loss: 1.8346 - val_acc: 0.4519\n",
      "Epoch 6/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.9881 - acc: 0.4114 - val_loss: 1.8016 - val_acc: 0.4685\n",
      "Epoch 7/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 1.9491 - acc: 0.4228 - val_loss: 1.7800 - val_acc: 0.4790\n",
      "Epoch 8/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 1.9354 - acc: 0.4314 - val_loss: 1.7751 - val_acc: 0.4739\n",
      "Epoch 9/30\n",
      "46630/46630 [==============================] - 1s 28us/step - loss: 1.9219 - acc: 0.4360 - val_loss: 1.7751 - val_acc: 0.4795\n",
      "Epoch 10/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.9088 - acc: 0.4437 - val_loss: 1.7575 - val_acc: 0.4871\n",
      "Epoch 11/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.8994 - acc: 0.4452 - val_loss: 1.7808 - val_acc: 0.4811\n",
      "Epoch 12/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.8914 - acc: 0.4498 - val_loss: 1.7753 - val_acc: 0.4919\n",
      "Epoch 13/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 1.8939 - acc: 0.4511 - val_loss: 1.7726 - val_acc: 0.4841\n",
      "Epoch 14/30\n",
      "46630/46630 [==============================] - 1s 31us/step - loss: 1.8949 - acc: 0.4494 - val_loss: 1.8041 - val_acc: 0.4866\n",
      "Epoch 15/30\n",
      "46630/46630 [==============================] - 1s 28us/step - loss: 1.9078 - acc: 0.4539 - val_loss: 1.8014 - val_acc: 0.4852\n",
      "Epoch 16/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.9179 - acc: 0.4521 - val_loss: 1.8204 - val_acc: 0.4883\n",
      "Epoch 17/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 1.9207 - acc: 0.4522 - val_loss: 1.8428 - val_acc: 0.4786\n",
      "Epoch 18/30\n",
      "46630/46630 [==============================] - 1s 31us/step - loss: 1.9339 - acc: 0.4528 - val_loss: 1.8842 - val_acc: 0.4804\n",
      "Epoch 19/30\n",
      "46630/46630 [==============================] - 1s 31us/step - loss: 1.9351 - acc: 0.4503 - val_loss: 1.9166 - val_acc: 0.4678\n",
      "Epoch 20/30\n",
      "46630/46630 [==============================] - 1s 32us/step - loss: 1.9569 - acc: 0.4473 - val_loss: 1.9191 - val_acc: 0.4752\n",
      "Epoch 21/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 1.9613 - acc: 0.4480 - val_loss: 1.9137 - val_acc: 0.4709\n",
      "Epoch 22/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.9634 - acc: 0.4481 - val_loss: 1.9587 - val_acc: 0.4667\n",
      "Epoch 23/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 1.9928 - acc: 0.4439 - val_loss: 1.9706 - val_acc: 0.4698\n",
      "Epoch 24/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 1.9933 - acc: 0.4441 - val_loss: 1.9817 - val_acc: 0.4671\n",
      "Epoch 25/30\n",
      "46630/46630 [==============================] - 1s 29us/step - loss: 2.0149 - acc: 0.4417 - val_loss: 2.0143 - val_acc: 0.4710\n",
      "Epoch 26/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 2.0215 - acc: 0.4398 - val_loss: 1.9884 - val_acc: 0.4552\n",
      "Epoch 27/30\n",
      "46630/46630 [==============================] - 1s 30us/step - loss: 2.0494 - acc: 0.4338 - val_loss: 2.0314 - val_acc: 0.4638\n",
      "Epoch 28/30\n",
      "46630/46630 [==============================] - 1s 27us/step - loss: 2.0464 - acc: 0.4347 - val_loss: 2.0626 - val_acc: 0.4596\n",
      "Epoch 29/30\n",
      "46630/46630 [==============================] - 1s 31us/step - loss: 2.0699 - acc: 0.4325 - val_loss: 2.1051 - val_acc: 0.4653\n",
      "Epoch 30/30\n",
      "46630/46630 [==============================] - 1s 31us/step - loss: 2.0998 - acc: 0.4274 - val_loss: 2.0798 - val_acc: 0.4612\n",
      "CPU times: user 1min 4s, sys: 8.31 s, total: 1min 12s\n",
      "Wall time: 42.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4e5060a208>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "asr.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio VAE",
   "language": "python",
   "name": "audio_vae_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
